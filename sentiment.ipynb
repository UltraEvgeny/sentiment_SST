{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from torchtext.datasets import IMDB, SST\n",
    "from torchtext.data import Field, LabelField, BucketIterator\n",
    "import torchtext\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.width = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For sentiment analysis we will be using SST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size and label count (%), n=6835\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.422531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.387418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.190051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  \n",
       "positive  0.422531\n",
       "negative  0.387418\n",
       "neutral   0.190051"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val data size and label count (%), n=1709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.422469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.387361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.190170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  \n",
       "positive  0.422469\n",
       "negative  0.387361\n",
       "neutral   0.190170"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size and label count (%), n=2210\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.412670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.411312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>0.176018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  \n",
       "negative  0.412670\n",
       "positive  0.411312\n",
       "neutral   0.176018"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SRC = Field()\n",
    "TRG = LabelField(dtype=torch.int64)\n",
    "sp = spacy.load('en')\n",
    "\n",
    "\n",
    "train_data, test_data = SST.splits(SRC, TRG, validation=None)\n",
    "train_data, val_data = train_data.split(0.8, (torch.Generator().manual_seed(10), ))\n",
    "\n",
    "print(f'Train data size and label count (%), n={len(train_data)}')\n",
    "display(pd.Series([x.label for x in train_data.examples]).value_counts(normalize=True).rename('').to_frame())\n",
    "print(f'Val data size and label count (%), n={len(val_data)}')\n",
    "display(pd.Series([x.label for x in val_data.examples]).value_counts(normalize=True).rename('').to_frame())\n",
    "print(f'Test data size and label count (%), n={len(test_data)}')\n",
    "display(pd.Series([x.label for x in test_data.examples]).value_counts(normalize=True).rename('').to_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'freqs': Counter({'positive': 2888, 'negative': 2648, 'neutral': 1299}), 'itos': ['positive', 'negative', 'neutral'], 'unk_index': None, 'stoi': defaultdict(None, {'positive': 0, 'negative': 1, 'neutral': 2}), 'vectors': None}\n",
      "Unique tokens in source vocabulary: 2927\n",
      "Unique tokens in TRG vocabulary: 3\n"
     ]
    }
   ],
   "source": [
    "SRC.build_vocab(train_data, max_size=10000, min_freq=5, vectors=\"glove.6B.100d\")  # using pretrained word embedding\n",
    "TRG.build_vocab(train_data, min_freq=5)\n",
    "\n",
    "print(vars(TRG.vocab))\n",
    "print(f\"Unique tokens in source vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in TRG vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = 300\n",
    "train_iterator, val_iterator, test_iterator = BucketIterator.splits(\n",
    "      (train_data, val_data, test_data),\n",
    "      batch_size=BATCH_SIZE,\n",
    "      device=device\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GRU NN (we use GRU now, because it showed better accuracy on this dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        # input_dim <--- vocabulary size\n",
    "        # output_dim <--- len ([positive, negative]) == 2\n",
    "        # emb_dim <--- embedding dimension of embedding matrix\n",
    "\n",
    "        super(GRU, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # shape: [source_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))  # sahpe: [src_len, batch_size, embed_dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        # output[batch, hidden_dim]\n",
    "        # hiddden[n_layers, batch, hidden_dim]\n",
    "        output = self.fc1(output[-1])\n",
    "        output = self.fc2(self.relu(output))\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM = 256\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "model = GRU(INPUT_DIM, OUTPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
    "model.embedding.weight.data.copy_(SRC.vocab.vectors)\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training function\n",
    "def train(model, iterator, optimizer=optimizer, criterion=criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.text.to(device)\n",
    "        trg = batch.label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    mean_loss = epoch_loss / len(iterator)\n",
    "    scheduler.step(mean_loss)\n",
    "    return mean_loss  # mean loss\n",
    "\n",
    "\n",
    "def check_accuracy(data_iterator, model):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_iterator):\n",
    "            src = batch.text.to(device)\n",
    "            trg = batch.label.to(device)\n",
    "            output = model(src)\n",
    "\n",
    "            total_correct += torch.sum(torch.eq(output.argmax(1), trg))\n",
    "            total_count += len(trg)\n",
    "\n",
    "    return f'{total_correct}/{total_count}', round(float(total_correct/total_count), 3)\n",
    "\n",
    "\n",
    "def do_prediction(sentence):\n",
    "\n",
    "    if type(sentence) == str:\n",
    "        tokanised_sentence = [word.text for word in sp.tokenizer(sentence)]\n",
    "    else:\n",
    "        tokanised_sentence = sentence\n",
    "\n",
    "    input_data = [SRC.vocab.stoi[word.lower()] for word in tokanised_sentence]\n",
    "    input_data = torch.tensor(input_data, dtype=torch.int64).unsqueeze(1).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    output = model(input_data)\n",
    "    label_mapping = train_data.fields['label'].vocab.stoi\n",
    "    r = {'text': sentence, **{k: v for k, v in zip(sorted(label_mapping, key=lambda x: label_mapping[x]), output[0].tolist())}}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 --> 1.055275144784347\n",
      "Epoch 1 --> 1.051476214243018\n",
      "Epoch 2 --> 1.048110158547111\n",
      "Epoch 3 --> 1.0495818438737288\n",
      "Epoch 4 --> 1.0480802421984465\n",
      "Epoch 5 --> 1.0476576556330142\n",
      "Epoch 6 --> 1.0477439165115356\n",
      "Epoch 7 --> 1.031999743503073\n",
      "Epoch 8 --> 0.9405067874037701\n",
      "Epoch 9 --> 0.8484045344850292\n",
      "Epoch 10 --> 0.7865908171819604\n",
      "Epoch 11 --> 0.7268832325935364\n",
      "Epoch 12 --> 0.6954181764436804\n",
      "Epoch 13 --> 0.6643368161242941\n",
      "Epoch 14 --> 0.6369987985362178\n",
      "Epoch 15 --> 0.615580255570619\n",
      "Epoch 16 --> 0.6121622479480245\n",
      "Epoch 17 --> 0.5638920936895453\n",
      "Epoch 18 --> 0.5509859880675441\n",
      "Epoch 19 --> 0.5286451526310133\n",
      "Epoch 20 --> 0.49738914292791614\n",
      "Epoch 21 --> 0.477946981139805\n",
      "Epoch 22 --> 0.4571795554264732\n",
      "Epoch 23 --> 0.43299377612445666\n",
      "Epoch 24 --> 0.4277369431827379\n",
      "Epoch 25 --> 0.40416069263997284\n",
      "Epoch 26 --> 0.3864903890568277\n",
      "Epoch 27 --> 0.3590003407519797\n",
      "Epoch 28 --> 0.3592718930348106\n",
      "Epoch 29 --> 0.3268903584583946\n",
      "Epoch 30 --> 0.312598405972771\n",
      "Epoch 31 --> 0.2910182670406673\n",
      "Epoch 32 --> 0.29064181382241455\n",
      "Epoch 33 --> 0.26118371149768\n",
      "Epoch 34 --> 0.2482978325823079\n",
      "Epoch 35 --> 0.2263603184534156\n",
      "Epoch 36 --> 0.22392224099325098\n",
      "Epoch 37 --> 0.20941682483838953\n",
      "Epoch 38 --> 0.19198247855124267\n",
      "Epoch 39 --> 0.18890856854293658\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy_str</th>\n",
       "      <th>val_accuracy_str</th>\n",
       "      <th>test_accuracy_str</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2888/6835</td>\n",
       "      <td>722/1709</td>\n",
       "      <td>909/2210</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3856/6835</td>\n",
       "      <td>900/1709</td>\n",
       "      <td>1269/2210</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4035/6835</td>\n",
       "      <td>998/1709</td>\n",
       "      <td>1279/2210</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4656/6835</td>\n",
       "      <td>1047/1709</td>\n",
       "      <td>1397/2210</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4786/6835</td>\n",
       "      <td>1049/1709</td>\n",
       "      <td>1443/2210</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4950/6835</td>\n",
       "      <td>1099/1709</td>\n",
       "      <td>1457/2210</td>\n",
       "      <td>0.724</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5027/6835</td>\n",
       "      <td>1087/1709</td>\n",
       "      <td>1471/2210</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5115/6835</td>\n",
       "      <td>1096/1709</td>\n",
       "      <td>1474/2210</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5190/6835</td>\n",
       "      <td>1093/1709</td>\n",
       "      <td>1458/2210</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5202/6835</td>\n",
       "      <td>1084/1709</td>\n",
       "      <td>1456/2210</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5252/6835</td>\n",
       "      <td>1093/1709</td>\n",
       "      <td>1463/2210</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5516/6835</td>\n",
       "      <td>1067/1709</td>\n",
       "      <td>1444/2210</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5464/6835</td>\n",
       "      <td>1065/1709</td>\n",
       "      <td>1425/2210</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5679/6835</td>\n",
       "      <td>1024/1709</td>\n",
       "      <td>1353/2210</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5715/6835</td>\n",
       "      <td>1050/1709</td>\n",
       "      <td>1421/2210</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5812/6835</td>\n",
       "      <td>1043/1709</td>\n",
       "      <td>1394/2210</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5962/6835</td>\n",
       "      <td>1036/1709</td>\n",
       "      <td>1416/2210</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5981/6835</td>\n",
       "      <td>1031/1709</td>\n",
       "      <td>1359/2210</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6143/6835</td>\n",
       "      <td>1018/1709</td>\n",
       "      <td>1349/2210</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6206/6835</td>\n",
       "      <td>1042/1709</td>\n",
       "      <td>1370/2210</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6240/6835</td>\n",
       "      <td>1020/1709</td>\n",
       "      <td>1380/2210</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6372/6835</td>\n",
       "      <td>1011/1709</td>\n",
       "      <td>1403/2210</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>6393/6835</td>\n",
       "      <td>1005/1709</td>\n",
       "      <td>1374/2210</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6449/6835</td>\n",
       "      <td>1021/1709</td>\n",
       "      <td>1366/2210</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.597</td>\n",
       "      <td>0.618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>6465/6835</td>\n",
       "      <td>1047/1709</td>\n",
       "      <td>1409/2210</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6554/6835</td>\n",
       "      <td>1008/1709</td>\n",
       "      <td>1354/2210</td>\n",
       "      <td>0.959</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6542/6835</td>\n",
       "      <td>1043/1709</td>\n",
       "      <td>1395/2210</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6602/6835</td>\n",
       "      <td>1028/1709</td>\n",
       "      <td>1389/2210</td>\n",
       "      <td>0.966</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6637/6835</td>\n",
       "      <td>1019/1709</td>\n",
       "      <td>1370/2210</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6670/6835</td>\n",
       "      <td>1033/1709</td>\n",
       "      <td>1369/2210</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6628/6835</td>\n",
       "      <td>1050/1709</td>\n",
       "      <td>1400/2210</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6699/6835</td>\n",
       "      <td>1039/1709</td>\n",
       "      <td>1377/2210</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6698/6835</td>\n",
       "      <td>1032/1709</td>\n",
       "      <td>1371/2210</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6733/6835</td>\n",
       "      <td>1013/1709</td>\n",
       "      <td>1348/2210</td>\n",
       "      <td>0.985</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy_str val_accuracy_str test_accuracy_str  train_accuracy  val_accuracy  test_accuracy\n",
       "0           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "1           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "2           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "3           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "4           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "5           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "6           2888/6835         722/1709          909/2210           0.423         0.422          0.411\n",
       "7           3856/6835         900/1709         1269/2210           0.564         0.527          0.574\n",
       "8           4035/6835         998/1709         1279/2210           0.590         0.584          0.579\n",
       "9           4656/6835        1047/1709         1397/2210           0.681         0.613          0.632\n",
       "10          4786/6835        1049/1709         1443/2210           0.700         0.614          0.653\n",
       "11          4950/6835        1099/1709         1457/2210           0.724         0.643          0.659\n",
       "12          5027/6835        1087/1709         1471/2210           0.735         0.636          0.666\n",
       "13          5115/6835        1096/1709         1474/2210           0.748         0.641          0.667\n",
       "14          5190/6835        1093/1709         1458/2210           0.759         0.640          0.660\n",
       "15          5202/6835        1084/1709         1456/2210           0.761         0.634          0.659\n",
       "16          5252/6835        1093/1709         1463/2210           0.768         0.640          0.662\n",
       "17          5516/6835        1067/1709         1444/2210           0.807         0.624          0.653\n",
       "18          5464/6835        1065/1709         1425/2210           0.799         0.623          0.645\n",
       "19          5679/6835        1024/1709         1353/2210           0.831         0.599          0.612\n",
       "20          5715/6835        1050/1709         1421/2210           0.836         0.614          0.643\n",
       "21          5812/6835        1043/1709         1394/2210           0.850         0.610          0.631\n",
       "22          5962/6835        1036/1709         1416/2210           0.872         0.606          0.641\n",
       "23          5981/6835        1031/1709         1359/2210           0.875         0.603          0.615\n",
       "24          6143/6835        1018/1709         1349/2210           0.899         0.596          0.610\n",
       "25          6206/6835        1042/1709         1370/2210           0.908         0.610          0.620\n",
       "26          6240/6835        1020/1709         1380/2210           0.913         0.597          0.624\n",
       "27          6372/6835        1011/1709         1403/2210           0.932         0.592          0.635\n",
       "28          6393/6835        1005/1709         1374/2210           0.935         0.588          0.622\n",
       "29          6449/6835        1021/1709         1366/2210           0.944         0.597          0.618\n",
       "30          6465/6835        1047/1709         1409/2210           0.946         0.613          0.638\n",
       "31          6554/6835        1008/1709         1354/2210           0.959         0.590          0.613\n",
       "32          6542/6835        1043/1709         1395/2210           0.957         0.610          0.631\n",
       "33          6602/6835        1028/1709         1389/2210           0.966         0.602          0.629\n",
       "34          6637/6835        1019/1709         1370/2210           0.971         0.596          0.620\n",
       "35          6670/6835        1033/1709         1369/2210           0.976         0.604          0.619\n",
       "36          6628/6835        1050/1709         1400/2210           0.970         0.614          0.633\n",
       "37          6699/6835        1039/1709         1377/2210           0.980         0.608          0.623\n",
       "38          6698/6835        1032/1709         1371/2210           0.980         0.604          0.620\n",
       "39          6733/6835        1013/1709         1348/2210           0.985         0.593          0.610"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best epoch (11) result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_accuracy_str</th>\n",
       "      <td>4950/6835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_accuracy_str</th>\n",
       "      <td>1099/1709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy_str</th>\n",
       "      <td>1457/2210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_accuracy</th>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val_accuracy</th>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_accuracy</th>\n",
       "      <td>0.659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Best epoch (11) result\n",
       "train_accuracy_str              4950/6835\n",
       "val_accuracy_str                1099/1709\n",
       "test_accuracy_str               1457/2210\n",
       "train_accuracy                      0.724\n",
       "val_accuracy                        0.643\n",
       "test_accuracy                       0.659"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model on epoch=11\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "total_epoch = 40\n",
    "for epoch in range(total_epoch):\n",
    "    result = train(model=model, iterator=train_iterator)\n",
    "    print(f'Epoch {epoch} -->', result)\n",
    "    train_check = check_accuracy(train_iterator, model)\n",
    "    val_check = check_accuracy(val_iterator, model)\n",
    "    test_check = check_accuracy(test_iterator, model)\n",
    "    s = pd.Series({'train_accuracy_str': train_check[0],\n",
    "                   'val_accuracy_str': val_check[0],\n",
    "                   'test_accuracy_str': test_check[0],\n",
    "                   'train_accuracy': train_check[1],\n",
    "                   'val_accuracy': val_check[1],\n",
    "                   'test_accuracy': test_check[1],\n",
    "                   }, name=epoch)\n",
    "    df = df.append(s)[s.index.tolist()]\n",
    "    torch.save(model.state_dict(), os.path.join('model_snapshots', f'{epoch}.pth'))\n",
    "\n",
    "display(df)\n",
    "best_epoch = df['val_accuracy'].idxmax()\n",
    "\n",
    "display(df.loc[best_epoch].rename(f'Best epoch ({best_epoch}) result').to_frame())\n",
    "\n",
    "print(f'Loading model on epoch={best_epoch}')\n",
    "model.load_state_dict(torch.load(f'model_snapshots/{best_epoch}.pth'))\n",
    "\n",
    "for e in range(total_epoch):\n",
    "    os.remove(f'model_snapshots/{e}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That was great!</td>\n",
       "      <td>1.180104</td>\n",
       "      <td>-0.411761</td>\n",
       "      <td>-0.529966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That was very bad!</td>\n",
       "      <td>-1.181618</td>\n",
       "      <td>1.398464</td>\n",
       "      <td>0.069267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is wonderful film. I like it!</td>\n",
       "      <td>3.511635</td>\n",
       "      <td>-3.602530</td>\n",
       "      <td>-1.794998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Terrible, stupid, tasteless! Worst thing I've ...</td>\n",
       "      <td>-1.990963</td>\n",
       "      <td>2.411172</td>\n",
       "      <td>0.200588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Don't know, what to say.</td>\n",
       "      <td>-0.793966</td>\n",
       "      <td>0.995930</td>\n",
       "      <td>-0.002925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  positive  negative   neutral\n",
       "0                                    That was great!  1.180104 -0.411761 -0.529966\n",
       "1                                 That was very bad! -1.181618  1.398464  0.069267\n",
       "2                   It is wonderful film. I like it!  3.511635 -3.602530 -1.794998\n",
       "3  Terrible, stupid, tasteless! Worst thing I've ... -1.990963  2.411172  0.200588\n",
       "4                           Don't know, what to say. -0.793966  0.995930 -0.002925"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_predict = [\n",
    "    'That was great!',\n",
    "    'That was very bad!',\n",
    "    'It is wonderful film. I like it!',\n",
    "    \"Terrible, stupid, tasteless! Worst thing I've ever seen\",\n",
    "    \"Don't know, what to say.\",\n",
    "]\n",
    "\n",
    "display(pd.DataFrame([do_prediction(p) for p in to_predict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
